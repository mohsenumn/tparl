{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "residential-checkout",
      "metadata": {
        "id": "residential-checkout"
      },
      "source": [
        "# Lazy RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sporting-williams",
      "metadata": {
        "id": "sporting-williams"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "satisfactory-focus",
      "metadata": {
        "id": "satisfactory-focus"
      },
      "source": [
        "So we saw in the last lesson that logs each of the steps taken when we transform or query our data.  It turns out that Spark plans out what steps are needed to be taken in advance, and then executes the function.  This way, with some planning, it can efficiently operate on our data. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forced-georgia",
      "metadata": {
        "id": "forced-georgia"
      },
      "source": [
        "We'll learn more about how spark achieves this efficiency in this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "animated-appreciation",
      "metadata": {
        "id": "animated-appreciation"
      },
      "source": [
        "### Getting Setup (On Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engaged-contrast",
      "metadata": {
        "id": "engaged-contrast"
      },
      "source": [
        "* Begin by installing some pip packages and the java development kit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "specified-virginia",
      "metadata": {
        "id": "specified-virginia"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -U -q PyDrive --quiet \n",
        "!apt install openjdk-8-jdk-headless &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entire-laptop",
      "metadata": {
        "id": "entire-laptop"
      },
      "source": [
        "* Then set the java environmental variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adjacent-archives",
      "metadata": {
        "id": "adjacent-archives"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "homeless-beginning",
      "metadata": {
        "id": "homeless-beginning"
      },
      "source": [
        "* Then connect to a SparkSession, setting the spark ui port to `4050`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "increasing-preliminary",
      "metadata": {
        "id": "increasing-preliminary"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[2]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparative-restriction",
      "metadata": {
        "id": "comparative-restriction"
      },
      "source": [
        "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "disabled-radius",
      "metadata": {
        "id": "disabled-radius"
      },
      "outputs": [],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "proper-longer",
      "metadata": {
        "id": "proper-longer"
      },
      "source": [
        "* And finally we get a link our Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "buried-blink",
      "metadata": {
        "id": "buried-blink"
      },
      "outputs": [],
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opposed-oliver",
      "metadata": {
        "id": "opposed-oliver"
      },
      "source": [
        "### Looking Under the Hood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "logical-catholic",
      "metadata": {
        "id": "logical-catholic"
      },
      "source": [
        "Now  let's again create an RDD from our movie records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "charitable-assets",
      "metadata": {
        "id": "charitable-assets"
      },
      "outputs": [],
      "source": [
        "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optimum-terror",
      "metadata": {
        "id": "optimum-terror",
        "outputId": "6526f116-94c8-4768-bb0d-879da64c3495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd = sc.parallelize(movies)\n",
        "movies_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entertaining-investor",
      "metadata": {
        "id": "entertaining-investor"
      },
      "source": [
        "And then let's capitalize the movies, and select the movies that begin with `d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "frozen-retail",
      "metadata": {
        "id": "frozen-retail",
        "outputId": "f97fd033-54dc-4a3f-af10-03dd152f9711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knowing-bleeding",
      "metadata": {
        "id": "knowing-bleeding"
      },
      "source": [
        "Now as we know, Spark will partition the dataset across the cores of the executors, and then map through the records in parallel, returning all of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bronze-friend",
      "metadata": {
        "id": "bronze-friend"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/parallel.png?raw=1\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-processor",
      "metadata": {
        "id": "accredited-processor"
      },
      "source": [
        "Now let's change the function so that this time, instead of returning all of the results, we just return the first result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floating-louis",
      "metadata": {
        "id": "floating-louis",
        "outputId": "bfa30ca0-51c9-4df5-fc09-a485d21f8635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "expired-leave",
      "metadata": {
        "id": "expired-leave"
      },
      "source": [
        "Now if we think about, this previous step, here we would not have to map through all of the steps just to return a single result.  And it turns out if we look at Spark, we can see that even though the dataset was distributed -- it only needed to perform work on a single partition to return one result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accepted-drink",
      "metadata": {
        "id": "accepted-drink"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/individual_task.png?raw=1\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "herbal-future",
      "metadata": {
        "id": "herbal-future"
      },
      "source": [
        "This ability, to see the end result that needs to be returned, and to work efficiently to only take the needed steps to return those results, is a valuable feature when working with large datasets.  And we can better see how Spark accomplishes it in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitting-publisher",
      "metadata": {
        "id": "fitting-publisher"
      },
      "source": [
        "### A little experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electronic-engagement",
      "metadata": {
        "id": "electronic-engagement"
      },
      "source": [
        "If we run the code below, notice that nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "powered-bikini",
      "metadata": {
        "id": "powered-bikini",
        "outputId": "e7c95958-1121-4d4a-ded6-8cad2581f87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[8] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-surprise",
      "metadata": {
        "id": "published-surprise"
      },
      "source": [
        "And even if we chain the map and the filter methods, still nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exceptional-saturn",
      "metadata": {
        "id": "exceptional-saturn",
        "outputId": "97b139a6-412a-4f83-c509-ca8c2f250495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[3] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fossil-toner",
      "metadata": {
        "id": "fossil-toner"
      },
      "source": [
        "It's only when we add a collect function on the end, will some data be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "variable-print",
      "metadata": {
        "id": "variable-print",
        "outputId": "ad9ace36-5404-4ef6-e701-3ebcc2850edb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.filter(lambda movie: movie[0] == 'd').map(lambda movie: movie.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unlimited-civilian",
      "metadata": {
        "id": "unlimited-civilian"
      },
      "source": [
        "So above, nothing was returned when we ran the `map` and `collect` functions, because when we only executed those functions, Spark did not actually act on the data.  Then in the third line we finally did act on the data.  We told Spark that we want to both transform, and filter the data, and then return all of the results.  \n",
        "\n",
        "So it's only when we called the `collect` function that Spark's driver determined the tasks to then send off to the executors and return the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olive-format",
      "metadata": {
        "id": "olive-format"
      },
      "source": [
        "### Transformations and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "governmental-definition",
      "metadata": {
        "id": "governmental-definition"
      },
      "source": [
        "So above we can see that the functions `map` and `filter` do not actually perform any work on our data.  Instead steps are only kicked off when we call the `collect` method.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flush-remains",
      "metadata": {
        "id": "flush-remains"
      },
      "source": [
        "In Spark, the methods that kick off tasks and return results are called **actions** (eg. collect).  And methods like `map` and `transform` that do not are called **transformations**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noted-exposure",
      "metadata": {
        "id": "noted-exposure"
      },
      "source": [
        "1. Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protected-biology",
      "metadata": {
        "id": "protected-biology"
      },
      "source": [
        "So we already saw that transformations include `map` and `filter`, and our transformations do not actually return results to our users.  Here's a couple other transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imported-writer",
      "metadata": {
        "id": "imported-writer"
      },
      "source": [
        "* sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intense-swedish",
      "metadata": {
        "id": "intense-swedish"
      },
      "source": [
        "The `sample` method allows us to take a random sample from our dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "major-familiar",
      "metadata": {
        "id": "major-familiar",
        "outputId": "9f73fc30-ee6a-4b17-c59f-93c74973af3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[16] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.sample(fraction = .2, withReplacement = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "considered-copper",
      "metadata": {
        "id": "considered-copper"
      },
      "source": [
        "> Notice that it does not return any data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "supreme-idaho",
      "metadata": {
        "id": "supreme-idaho"
      },
      "source": [
        "* distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handed-tennis",
      "metadata": {
        "id": "handed-tennis",
        "outputId": "9ceccc75-868f-416b-adc2-8ee79b0a0be6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[14] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frequent-accounting",
      "metadata": {
        "id": "frequent-accounting"
      },
      "source": [
        "> Distinct finds the unique results.  Notice that it also does not return data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sustainable-tension",
      "metadata": {
        "id": "sustainable-tension"
      },
      "source": [
        "Finally, we have already seen `map`, which provides a one to one transformation of our records, and `select` which filters our data.  In each case, our transformations do not return data to us. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "endless-columbus",
      "metadata": {
        "id": "endless-columbus"
      },
      "source": [
        "2. Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-parts",
      "metadata": {
        "id": "happy-parts"
      },
      "source": [
        "Actions are a bit more about the end result.  So far we've learned about `collect`, which returns *all* of the results of a series of transformations.  \n",
        "\n",
        "* Take\n",
        "\n",
        "We've also seen `take`, which limits our results to a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radical-spotlight",
      "metadata": {
        "id": "radical-spotlight",
        "outputId": "0d871925-8cdc-497d-827a-71c18155c91e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.distinct().take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-diagram",
      "metadata": {
        "id": "hindu-diagram"
      },
      "source": [
        "> So `take` is similar to the `LIMIT` function in SQL. Notice that here our records are returned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "active-lending",
      "metadata": {
        "id": "active-lending"
      },
      "source": [
        "* Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mathematical-advice",
      "metadata": {
        "id": "mathematical-advice",
        "outputId": "43557281-8bfa-41d6-d9ed-78903a81de49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "asian-hepatitis",
      "metadata": {
        "id": "asian-hepatitis"
      },
      "source": [
        "Count simply counts the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changing-orange",
      "metadata": {
        "id": "changing-orange"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iraqi-archive",
      "metadata": {
        "id": "iraqi-archive"
      },
      "source": [
        "So we can see that, our actions have a bit of finality to them.  To get a better sense of the transformation and action functions, it's worth looking at the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "special-driving",
      "metadata": {
        "id": "special-driving"
      },
      "source": [
        "### Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appointed-detroit",
      "metadata": {
        "id": "appointed-detroit"
      },
      "source": [
        "[Berkley White Paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
        "\n",
        "[Pyspark RDD Methods blog](https://www.nbshare.io/notebook/403283317/How-To-Analyze-Data-Using-Pyspark-RDD/)\n",
        "\n",
        "\n",
        "[Databricks Debugging Spark Streaming](https://docs.databricks.com/spark/latest/rdd-streaming/debugging-streaming-applications.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}