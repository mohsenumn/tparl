{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "personal-passing",
      "metadata": {
        "id": "personal-passing"
      },
      "source": [
        "# Spark Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "independent-concentration",
      "metadata": {
        "id": "independent-concentration"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blessed-syracuse",
      "metadata": {
        "id": "blessed-syracuse"
      },
      "source": [
        "Now that we know different aspects about how and when Spark RDDs process data, let's make sure we understand the different components of what occurs when we call a Spark action.  In short, a Spark action kicks off a job, which may consist of many stages, each of which may consist of many tasks.  Let's further define Spark jobs, stages and tasks in this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retained-quebec",
      "metadata": {
        "id": "retained-quebec"
      },
      "source": [
        "### Getting Setup (On Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "similar-adrian",
      "metadata": {
        "id": "similar-adrian"
      },
      "source": [
        "* Begin by installing some pip packages and the java development kit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optical-pursuit",
      "metadata": {
        "id": "optical-pursuit"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -U -q PyDrive --quiet \n",
        "!apt install openjdk-8-jdk-headless &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dressed-microwave",
      "metadata": {
        "id": "dressed-microwave"
      },
      "source": [
        "* Then set the java environmental variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gentle-institution",
      "metadata": {
        "id": "gentle-institution"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "contrary-legislation",
      "metadata": {
        "id": "contrary-legislation"
      },
      "source": [
        "* Then connect to a SparkSession, setting the spark ui port to `4050`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "artificial-digit",
      "metadata": {
        "id": "artificial-digit"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[2]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-reading",
      "metadata": {
        "id": "advanced-reading"
      },
      "source": [
        "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weighted-wales",
      "metadata": {
        "id": "weighted-wales"
      },
      "outputs": [],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pacific-prince",
      "metadata": {
        "id": "pacific-prince"
      },
      "source": [
        "* And finally we get a link our Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-courage",
      "metadata": {
        "id": "mounted-courage"
      },
      "outputs": [],
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "egyptian-correction",
      "metadata": {
        "id": "egyptian-correction"
      },
      "source": [
        "### The Components of Calling an Action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "humanitarian-forwarding",
      "metadata": {
        "id": "humanitarian-forwarding"
      },
      "source": [
        "1. Spark Jobs \n",
        "\n",
        "So remember that Spark transformations do not actually act on our data, whereas actions do.  This means, if we look at our Spark UI, we'll see that the number Spark jobs is equal to the number of actions that we execute.  So if we simply call `movies_rdd.take(1)`, this kicks off a spark job."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colored-affairs",
      "metadata": {
        "id": "colored-affairs"
      },
      "source": [
        "2. Spark Stages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-machinery",
      "metadata": {
        "id": "daily-machinery"
      },
      "source": [
        "We also saw that a single job may have multiple stages.  We can think of our stages as a logical group of steps that can be completed at once.  For us, our stages are often divided into steps that can be performed before a shuffle, and then steps that can be performed after a shuffle.\n",
        "\n",
        "For example, let's use our spark context to create an rdd and then perform a `groupby`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "objective-discretion",
      "metadata": {
        "id": "objective-discretion"
      },
      "source": [
        "> And then create an RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "taken-identity",
      "metadata": {
        "id": "taken-identity"
      },
      "outputs": [],
      "source": [
        "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']\n",
        "movies_rdd = sc.parallelize(movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "traditional-affect",
      "metadata": {
        "id": "traditional-affect",
        "outputId": "d26c6723-7cf8-47f6-80c0-ad780ae10f51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('D', 2), ('P', 1), ('A', 1)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda word: word.title()). \\\n",
        "groupBy(lambda title: title[0]). \\\n",
        "map(lambda group: (group[0], len(group[1]))).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "higher-apparel",
      "metadata": {
        "id": "higher-apparel"
      },
      "source": [
        "We can see that the first stage involved reading the file, and grouping together the data.  This is often called the `ShuffleMapStage`, because it's where our shuffling and mapping occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rough-robin",
      "metadata": {
        "id": "rough-robin"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/preshuffle.png?raw=1\" width=\"30%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "million-italy",
      "metadata": {
        "id": "million-italy"
      },
      "source": [
        "And then once our data was grouped together properly, Spark could prepare the final results of counting the number of movies by record.  This is called the `ResultStage`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aboriginal-opinion",
      "metadata": {
        "id": "aboriginal-opinion"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/resultstage.png?raw=1\" width=\"35%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "contemporary-facial",
      "metadata": {
        "id": "contemporary-facial"
      },
      "source": [
        "3. Spark Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "downtown-landscape",
      "metadata": {
        "id": "downtown-landscape"
      },
      "source": [
        "We know that each stage is performed across multiple partitions of data simultaneously.  The processing of a stage that occurs on each separate partition is called a task.  We see this when we look in the event timeline of Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "macro-rehabilitation",
      "metadata": {
        "id": "macro-rehabilitation"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/tasks.png?raw=1\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overall-circulation",
      "metadata": {
        "id": "overall-circulation"
      },
      "source": [
        "Above is the tasks for a single stage, and we see that the same processing occurred across multiple partitions, with one task per partition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adapted-aluminum",
      "metadata": {
        "id": "adapted-aluminum"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "early-computer",
      "metadata": {
        "id": "early-computer"
      },
      "source": [
        "In this lesson, we saw how that spark job many have many stages and that a stage may be performed through many tasks.  There is one job per execution of an action in Spark.  And a job may contain many stages.  Oftentimes, if there is a shuffle, the stages are divided into the operations that occur before the shuffle, and the operations that occur after the shuffle -- the ShuffleMapStage and the ResultStage.  Finally, we saw that each stage can have many tasks, where we may have one task per partition of our data.  We can see these jobs, stages, and tasks through our Spark UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fossil-religious",
      "metadata": {
        "id": "fossil-religious"
      },
      "source": [
        "### Resources\n",
        "\n",
        "[Apache Spark Core Youtube](https://www.youtube.com/watch?v=_ArCesElWp8&ab_channel=Databricks)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}