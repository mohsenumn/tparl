{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03a8362",
   "metadata": {
    "id": "lonely-shaft"
   },
   "source": [
    "# Spark In Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ee57d",
   "metadata": {
    "id": "english-twelve"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbfd7c3",
   "metadata": {
    "id": "concerned-examination"
   },
   "source": [
    "If we think about our normal understanding of databases, our databases have two components:\n",
    "\n",
    "1. storage and\n",
    "2. processing\n",
    "\n",
    "That is, we both store data in our databases, and we can perform operations on that data.  \n",
    "\n",
    "As we'll see, with Spark we're mainly concerned with *processing our data*, and not with long term storage.  Instead, our data is stored to disk outside of the database, on a service like S3.  This has vast implications for the way we treat and value data in an organization, as well as the operations we can perform on that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b49aec",
   "metadata": {
    "id": "black-holly"
   },
   "source": [
    "### Diving In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4390c0",
   "metadata": {
    "id": "threaded-hardwood"
   },
   "source": [
    "Let's begin by just diving into Spark to see it in action.  From there we'll get a sense of how it operates differently than our previous databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4f8a4",
   "metadata": {
    "id": "expanded-party"
   },
   "source": [
    "> As we move along here, the goal is not to remember details of the steps.  We'll go through each of them in repeated detail later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd059c5",
   "metadata": {
    "id": "protected-links"
   },
   "source": [
    "So first we'll need to install some libraries for reading data from `s3`, and then we'll install Spark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a510c",
   "metadata": {
    "id": "sealed-differential"
   },
   "outputs": [],
   "source": [
    "# !/Users/jeff/opt/miniconda3/envs/data_engineering/bin/python3.8 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83060191",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "silent-investor",
    "outputId": "006685fa-d72f-40d3-e1a4-34db3b7d6dfb"
   },
   "outputs": [],
   "source": [
    "!pip install fsspec --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbc48a",
   "metadata": {
    "id": "precise-interest"
   },
   "source": [
    "> Then we run the following to connect to our spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc00b11",
   "metadata": {
    "id": "quarterly-sheet"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb67fa",
   "metadata": {
    "id": "reported-reasoning"
   },
   "source": [
    "Finally, we'll read in data from s3 using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504395ef",
   "metadata": {
    "id": "interior-match"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>Action</td>\n",
       "      <td>237000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>2787965087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>300000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>961000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>Action</td>\n",
       "      <td>245000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>880674609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Action</td>\n",
       "      <td>250000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>1084939099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>Action</td>\n",
       "      <td>260000000</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>284139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Pitch Black</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>23000000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>53187659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>Someone Like You...</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>23000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Her</td>\n",
       "      <td>Romance</td>\n",
       "      <td>23000000</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>47351251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>Joy Ride</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>10</td>\n",
       "      <td>36642838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>The Adventurer: The Curse of the Midas Box</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>25000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>6399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                       title      genre  \\\n",
       "0              0                                      Avatar     Action   \n",
       "1              1    Pirates of the Caribbean: At World's End  Adventure   \n",
       "2              2                                     Spectre     Action   \n",
       "3              3                       The Dark Knight Rises     Action   \n",
       "4              4                                 John Carter     Action   \n",
       "...          ...                                         ...        ...   \n",
       "1995        1995                                 Pitch Black   Thriller   \n",
       "1996        1996                         Someone Like You...     Comedy   \n",
       "1997        1997                                         Her    Romance   \n",
       "1998        1998                                    Joy Ride        NaN   \n",
       "1999        1999  The Adventurer: The Curse of the Midas Box    Fantasy   \n",
       "\n",
       "         budget  runtime  year  month     revenue  \n",
       "0     237000000    162.0  2009     12  2787965087  \n",
       "1     300000000    169.0  2007      5   961000000  \n",
       "2     245000000    148.0  2015     10   880674609  \n",
       "3     250000000    165.0  2012      7  1084939099  \n",
       "4     260000000    132.0  2012      3   284139100  \n",
       "...         ...      ...   ...    ...         ...  \n",
       "1995   23000000    108.0  2000      2    53187659  \n",
       "1996   23000000     97.0  2001      3           0  \n",
       "1997   23000000    126.0  2013     12    47351251  \n",
       "1998   23000000     97.0  2001     10    36642838  \n",
       "1999   25000000     99.0  2013     12        6399  \n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"s3://jigsaw-labs-student/imdb_movies.csv\")\n",
    "df = pd.read_csv('movies.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b737ea",
   "metadata": {
    "id": "decreased-samoa"
   },
   "source": [
    "And from there, we'll turn this dataframe into the equivalent of table in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40786619",
   "metadata": {
    "id": "entitled-excitement"
   },
   "outputs": [],
   "source": [
    "movies_df = spark.createDataFrame(df.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab566a19",
   "metadata": {
    "id": "common-burst",
    "outputId": "f5053f4d-6afb-4d0c-bb11-2698c56f13a1",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o87.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (infra04-wg011.eduroam.wireless.umn.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e1223069b46f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \"\"\"\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o87.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (infra04-wg011.eduroam.wireless.umn.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5d3b8",
   "metadata": {
    "id": "qualified-turning"
   },
   "source": [
    "### Spark stores data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf2bb2",
   "metadata": {
    "id": "responsible-washington"
   },
   "source": [
    "Ok, so now let's think about what happened above.  We just connected to our database, created the equivalent of a table to store our movies into, loaded in our data and then selected our first record.  \n",
    "\n",
    "This is a standard workflow in Spark.  We'll use an external resource like S3 for long term storage of our data.  And then we can quickly load in and query our data in Spark.  And unlike previous databases Spark, is primarily leaning on *memory* to process and store our data, as opposed to storing our data on *disk*.\n",
    "\n",
    "We can visualize our workflow in Spark like so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c1833",
   "metadata": {
    "id": "indirect-canadian"
   },
   "source": [
    "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/s3_to_movies.jpg?raw=1\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f078d",
   "metadata": {
    "id": "handmade-wonder"
   },
   "source": [
    "> One thing to clear up is that even though we'll use S3 to store our data, Spark will also read in and store that data as well.  But Spark will mainly store our data *in memory*.  And when we are done querying our data, we can shutdown the cluster, which deletes the data from Spark.  And when we perform another query, we'll just read in the data again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252a9ff",
   "metadata": {
    "id": "considered-failing"
   },
   "source": [
    "Finally, we should know that Spark is organized as a cluster.  So when Spark reads our data into memory, it can partition it onto different machines in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7848a63",
   "metadata": {
    "id": "valid-matthew"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/spark_cluster_partition.jpg?raw=1\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da70ac2",
   "metadata": {
    "id": "sapphire-logistics"
   },
   "source": [
    "### Contrasting with Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a3c0f",
   "metadata": {
    "id": "south-aerospace"
   },
   "source": [
    "So let's take a moment just to appreciate how, by (1) using an external resource for long term storage, and (2) storing data in memory within the cluster, Spark operates differently than other databases like postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470b6d",
   "metadata": {
    "id": "practical-success"
   },
   "source": [
    "With postgres, we store our data within the database and associated records are stored in a file.  When we perform an operation, like selecting records from a table, we then load data into memory and perform any transformations and return the result of that query to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c81a85",
   "metadata": {
    "id": "second-excitement"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/db_both.jpg?raw=1\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540353c",
   "metadata": {
    "id": "unavailable-draft"
   },
   "source": [
    "As we saw, by the time we load our data into Spark, these records, and our associated tables are primarily in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027edbd1",
   "metadata": {
    "id": "miniature-submission"
   },
   "source": [
    "### The Benefits of In Memory Storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaaa132",
   "metadata": {
    "id": "literary-afghanistan"
   },
   "source": [
    "So we just saw some of the architecture when working in Spark.  We use an external resource like S3 for long term storage, read our data into Spark which primarily stores our data in memory as opposed to storing the data on disk.  Now let's get into some of the benefits of this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5735409",
   "metadata": {
    "id": "assisted-vacation"
   },
   "source": [
    "1. Cheaper storage\n",
    "\n",
    "The first benefit is simply that it costs less money if we store our data externally on something like S3 than storing our data inside of a database on disk.  When our data is on S3, our data is simply written to disk, and there is minimal software running along with it.  By contrast, when we store our data in a database like postgres or redshift, for each node we have running, we are not just paying for the data storage but also a database with the capability to process that data.\n",
    "\n",
    "Because it's cheaper to store data in S3, we can store data that has relatively low value, or even unknown value.  And if a data scientist or data engineer can extract value from it later on, that data will be available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e1f0d",
   "metadata": {
    "id": "quarterly-dance"
   },
   "source": [
    "2. Schema on Read\n",
    "\n",
    "Not only is the data available, but with storage in memory, it's easier to explore and process this data.  For example, we saw above that it was relatively easy to load in and query our data in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e2cf8",
   "metadata": {
    "id": "straight-domain",
    "outputId": "09f586f7-78ec-4343-c425-aed00e222b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(title='Avatar', genre='Action', budget='237000000', runtime='162.0', year='2009', month='12', revenue='2787965087')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"s3://jigsaw-labs/imdb_movies.csv\")\n",
    "movies_df = spark.createDataFrame(df.astype(str))\n",
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce472c8",
   "metadata": {
    "id": "handy-spectrum"
   },
   "source": [
    "Contrast this with loading external data into our standard databases that store data on disk (eg. Postgres, MySQL).  There, to load in data we (1) need to create our tables, (2) transform our data to and (3) insert records into those tables.  With Spark, we can let the software determine the schema that fits to the data.  This is called `schema on read`, as the schema is determined when we read in the data.\n",
    "\n",
    "Also notice that we did not need to really transform the data before loading it into Spark.  For this reason, we do not need to follow the standard ETL pattern when loading in our data.  Instead we can first load our data, and then use Spark to transform it.  This is called Extract Load Transform (or ELT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a0c3d",
   "metadata": {
    "id": "american-facial"
   },
   "source": [
    "3. Memory intensive computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c96c9",
   "metadata": {
    "id": "residential-mumbai"
   },
   "source": [
    "The third benefit of memory storage really comes from the perspective of data science.  Performing certain computations in machine learning requires having a large amount of data accessible for computation, and having our entire dataset loaded in memory means that the data will be accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1ee35",
   "metadata": {
    "id": "subsequent-castle"
   },
   "source": [
    "### But there's still local some storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cee909",
   "metadata": {
    "id": "legal-texture"
   },
   "source": [
    "Even though Spark performs much of it's computation in memory, and primarily uses in memory storage, Spark nodes still do use local disks for data that does not fit into RAM, and to store intermediate output in a complex operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd99cdf",
   "metadata": {
    "id": "experimental-progress"
   },
   "source": [
    "> But this storage on disk is quite minor as compared to most databases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4ff64",
   "metadata": {
    "id": "constant-warren"
   },
   "source": [
    "So let's update our diagram to more accurately reflect the hardware spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff39c9d",
   "metadata": {
    "id": "exceptional-tourism"
   },
   "source": [
    "> <img src='https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/spark_cluster_disk.jpg?raw=1' width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f3cef",
   "metadata": {
    "id": "imperial-anxiety"
   },
   "source": [
    "So even though there is technically in local storage to disk Spark, what distinguishes Spark is it's reliance on in memory storage and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3cd79",
   "metadata": {
    "id": "asian-michigan"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780c75d",
   "metadata": {
    "id": "humanitarian-exception"
   },
   "source": [
    "In this lesson, we saw that with Spark we store our data externally in something like an S3 bucket, and then load this data into memory when used by Spark.  If we need additional memory, we add more nodes to our Spark cluster.\n",
    "\n",
    "Because Spark is primarily uses memory storage, this means that we are able to store data with low value, or unknown value.  And reading our data into memory means that we do not need to first create our data, transform our data and then load our data into these tables (ETL) but rather can load and then transform ouor data.  Finally, we saw that for machine learning, Spark is useful for memory intensive calculations. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
