{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "infrared-lucas",
      "metadata": {
        "id": "infrared-lucas"
      },
      "source": [
        "# Map Reduce and Shuffling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chicken-andrews",
      "metadata": {
        "id": "chicken-andrews"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intellectual-westminster",
      "metadata": {
        "id": "intellectual-westminster"
      },
      "source": [
        "In the last lesson, we saw how with Pyspark, we can partition our dataset across the cores of our executor.  This allows us to process data in a dataset in parallel.  In this lesson, we'll take a closer look at how Spark performs these operations across both nodes and cores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "banner-bosnia",
      "metadata": {
        "id": "banner-bosnia"
      },
      "source": [
        "### Getting Set Up (For Colab Only)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "searching-swing",
      "metadata": {
        "id": "searching-swing"
      },
      "source": [
        "Now, later on in this lesson, we'll be using the Spark UI -- which is a sort of Spark dashboard -- to learn about how Spark works under the hood.  If we're using google colab, we'll need to run the following to eventually connect to this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "governing-particular",
      "metadata": {
        "id": "governing-particular"
      },
      "source": [
        "* Begin by installing some pip packages and the java development kit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modified-pierre",
      "metadata": {
        "id": "modified-pierre"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -U -q PyDrive --quiet \n",
        "!apt install openjdk-8-jdk-headless &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "descending-amsterdam",
      "metadata": {
        "id": "descending-amsterdam"
      },
      "source": [
        "* Then set the java environmental variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-pressing",
      "metadata": {
        "id": "solved-pressing"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dangerous-bosnia",
      "metadata": {
        "id": "dangerous-bosnia"
      },
      "source": [
        "* Then connect to a SparkSession, setting the spark ui port to `4050`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sustained-treatment",
      "metadata": {
        "id": "sustained-treatment"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[2]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "direct-finger",
      "metadata": {
        "id": "direct-finger"
      },
      "source": [
        "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-panic",
      "metadata": {
        "id": "intellectual-panic"
      },
      "outputs": [],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caring-integral",
      "metadata": {
        "id": "caring-integral"
      },
      "source": [
        "* And finally we get a link our Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "every-christianity",
      "metadata": {
        "id": "every-christianity"
      },
      "outputs": [],
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chinese-desktop",
      "metadata": {
        "id": "chinese-desktop"
      },
      "source": [
        "### Map Reduce in Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neither-pregnancy",
      "metadata": {
        "id": "neither-pregnancy"
      },
      "source": [
        "Now from here, we'll create our list of movies in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "central-petersburg",
      "metadata": {
        "id": "central-petersburg"
      },
      "outputs": [],
      "source": [
        "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
        "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
        "          'Sicario']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gothic-friend",
      "metadata": {
        "id": "gothic-friend"
      },
      "source": [
        "And from there turn it into an RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accredited-nancy",
      "metadata": {
        "id": "accredited-nancy"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize(movies, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lonely-dominican",
      "metadata": {
        "id": "lonely-dominican"
      },
      "source": [
        "Now, we haven't said so before but when we pass our data into spark using the `parellize` method, we create a resiliant distributed dataset -- a RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfied-politics",
      "metadata": {
        "id": "satisfied-politics",
        "outputId": "90b9acb6-cc66-48c8-968c-26db9459abd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deluxe-anderson",
      "metadata": {
        "id": "deluxe-anderson"
      },
      "source": [
        "As we saw in previous lessons, an RDD is a dataset that is distributed across cores in our execeutor.  And as we know, if we say use a `filter` to look for the movie 'Michael Clayton' across the entire dataset, we perform this filter task four times across the cluster in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "downtown-population",
      "metadata": {
        "id": "downtown-population"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/map_red_cluster.jpg?raw=1\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empirical-graduate",
      "metadata": {
        "id": "empirical-graduate"
      },
      "source": [
        "What we see from the diagram above is that we send this query to our driver, and then the driver instructs the executors to query their partition of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brilliant-gauge",
      "metadata": {
        "id": "brilliant-gauge"
      },
      "source": [
        "So each core finds the matching records in it's relevant partition and returns the matched records to the spark driver, which returns them to us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liberal-undergraduate",
      "metadata": {
        "id": "liberal-undergraduate",
        "outputId": "f64d565c-329d-4134-9c28-45864af6291a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Michael Clayton']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.filter(lambda movie: movie =='Michael Clayton').collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conservative-characteristic",
      "metadata": {
        "id": "conservative-characteristic"
      },
      "source": [
        "If we perform a `map` operation, as opposed to a `filter` essentially the same thing occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handmade-diagram",
      "metadata": {
        "id": "handmade-diagram",
        "outputId": "f083dc1f-3933-443f-844e-b1f7728ca4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['SHAZAM!',\n",
              " 'MINARI',\n",
              " 'CAPTAIN MARVEL',\n",
              " 'PULP FICTION',\n",
              " 'CASABLANCA',\n",
              " 'MICHAEL CLAYTON',\n",
              " 'SICARIO']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.map(lambda movie: movie.upper()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "original-advance",
      "metadata": {
        "id": "original-advance"
      },
      "source": [
        "For example, above, we are capitalizing our data across four parts of our dataset simultaneously, and then gathering the sending the results from each partition to our driver, where our driver then sends the results back to us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "underlying-gilbert",
      "metadata": {
        "id": "underlying-gilbert"
      },
      "source": [
        "> This process of having each partition performing the same operation, and then combining the results is called `map reduce`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alpha-mirror",
      "metadata": {
        "id": "alpha-mirror"
      },
      "source": [
        "### Shuffling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-theme",
      "metadata": {
        "id": "material-theme"
      },
      "source": [
        "Now one thing we should be aware when when performing our distributed operations is shuffling.  Shuffling occurs when an operation requires us to send our data across partitions to successfully perform a query.  Because this transmission of data can require sending data across worker nodes, it is often a time intensive.\n",
        "\n",
        "So now let's talk about some operations that can cause shuffling.  Oftentimes, shuffling occurs when we need to perform a group by.  \n",
        "\n",
        "For example, let's say we have an RDD consisting of a list of movies, partitioned across cluster like so."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "modified-wagner",
      "metadata": {
        "id": "modified-wagner"
      },
      "source": [
        "* Partition 1\n",
        "    * Shazam!\n",
        "    * Minari\n",
        "* Partition 2\n",
        "    * Captain Marvel\n",
        "    * Pulp Fiction\n",
        "* Partition 3\n",
        "    * Casablanca\n",
        "    * Michael Clayton\n",
        "* Partition 4\n",
        "    * Sicario"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "genetic-husband",
      "metadata": {
        "id": "genetic-husband"
      },
      "source": [
        "So above is our distributed dataset, and now let's say our query is to group the movies together by their first letter.  Because some of the records we want grouped together start off on different nodes, this grouping will require sending some movies from one worker node to another so that they can reside together."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "different-blast",
      "metadata": {
        "id": "different-blast"
      },
      "source": [
        "> For example, above we can see that to group movies with the letter m together, we need to collect movies `Minari` and `Michael Clayton` across the first and third partitions, and then group them together. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "front-letter",
      "metadata": {
        "id": "front-letter"
      },
      "outputs": [],
      "source": [
        "grouped_rdd = rdd.groupBy(lambda movie: movie[0]).map(lambda x : (x[0], list(x[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twelve-extent",
      "metadata": {
        "id": "twelve-extent",
        "outputId": "128c3ca6-4223-4a44-e7ff-4dd9165e8b68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M', ['Minari', 'Michael Clayton']),\n",
              " ('S', ['Shazam!', 'Sicario']),\n",
              " ('C', ['Captain Marvel', 'Casablanca']),\n",
              " ('P', ['Pulp Fiction'])]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blind-suicide",
      "metadata": {
        "id": "blind-suicide"
      },
      "source": [
        "### Looking under the hood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gentle-jimmy",
      "metadata": {
        "id": "gentle-jimmy"
      },
      "source": [
        "Now we can get see some of these operations in action by looking at the Spark UI.  So let's reference our spark context and click on the link to the Spark UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oriented-fighter",
      "metadata": {
        "id": "oriented-fighter",
        "outputId": "edf303c3-a7e8-49a1-f3c4-8597053d06ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://jeffreys-air.home:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>films</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[2] appName=films>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hearing-dictionary",
      "metadata": {
        "id": "hearing-dictionary"
      },
      "source": [
        "And then if we click on the Spark UI link, we'll see something like the following. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "careful-democracy",
      "metadata": {
        "id": "careful-democracy"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/completed_jobs.png?raw=1\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weekly-slide",
      "metadata": {
        "id": "weekly-slide"
      },
      "source": [
        "So we can see that we have completed three jobs so far -- one for our `filter`, one for our `map`, and one for our `groupby`.  Now let's scroll down so we can begin to see more details about one of these jobs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adjusted-antarctica",
      "metadata": {
        "id": "adjusted-antarctica"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/jobs.png?raw=1\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secret-simple",
      "metadata": {
        "id": "secret-simple"
      },
      "source": [
        "The jobs are listed in reverse order - meaning that our most recent job is listed as job id 2, at the top of the list.  This was our group by job.  \n",
        "\n",
        "We can click on the blue link to see more details about the job."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dimensional-little",
      "metadata": {
        "id": "dimensional-little"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/staged_group_by.png?raw=1\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thousand-orchestra",
      "metadata": {
        "id": "thousand-orchestra"
      },
      "source": [
        "So if we click the drop down for `Dag visualization`, we can see a little bit more about what occurred in the job.  Let's look at Stage 2.  Stage 2 says `parallelize` -- telling us it involved running a task in parallel.  Then if we look at the below table to `Stage Id 2`, we can see that this parallel task involved our group by.  \n",
        "\n",
        "Ok, now let's click on the stage 2 box to see a little more details about this stage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "played-theater",
      "metadata": {
        "id": "played-theater"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/group_by_task.png?raw=1\" width=\"40%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "underlying-wednesday",
      "metadata": {
        "id": "underlying-wednesday"
      },
      "source": [
        "So here we can see that the first task in this stage is to read in the RDD.  And then the next task is to perform the group by.  If we scroll further down, we can see that it's in this stage that the shuffling occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "worthy-exposure",
      "metadata": {
        "id": "worthy-exposure"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/shuffling.png?raw=1\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laden-aluminum",
      "metadata": {
        "id": "laden-aluminum"
      },
      "source": [
        "The light green in the chart above is for the compute time -- searching through and querying the collection.  And the light yellow describes shuffling write time, so this is where we can see that the shuffling occur, when our dataset is reorganized by the first letter.  \n",
        "\n",
        "> The shuffle time is relatively small here as all partitions are located on our local computer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rolled-utilization",
      "metadata": {
        "id": "rolled-utilization"
      },
      "source": [
        "Remember that our job comes from the following code.  So the groupby seems to have occurred in Stage 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sustained-dependence",
      "metadata": {
        "id": "sustained-dependence",
        "outputId": "3aa38bff-f87c-472d-9982-0deef8f4a9f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M', <pyspark.resultiterable.ResultIterable at 0x10a4b0a30>),\n",
              " ('S', <pyspark.resultiterable.ResultIterable at 0x10a583550>),\n",
              " ('C', <pyspark.resultiterable.ResultIterable at 0x10a583130>),\n",
              " ('P', <pyspark.resultiterable.ResultIterable at 0x10a5834c0>)]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.groupBy(lambda movie: movie[0]).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "emotional-mortgage",
      "metadata": {
        "id": "emotional-mortgage"
      },
      "source": [
        "* map statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "different-commonwealth",
      "metadata": {
        "id": "different-commonwealth"
      },
      "source": [
        "And now that would leave the `map` for stage 3.  So let's take a look at that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "front-bride",
      "metadata": {
        "id": "front-bride",
        "outputId": "37bff7e7-31ba-427d-f77a-898eb3ca08af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M', ['Minari', 'Michael Clayton']),\n",
              " ('S', ['Shazam!', 'Sicario']),\n",
              " ('C', ['Captain Marvel', 'Casablanca']),\n",
              " ('P', ['Pulp Fiction'])]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.groupBy(lambda movie: movie[0]).map(lambda x : (x[0], list(x[1]))).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "necessary-budapest",
      "metadata": {
        "id": "necessary-budapest"
      },
      "source": [
        "So we can see these last steps represented if we click on stage 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "creative-ordinance",
      "metadata": {
        "id": "creative-ordinance"
      },
      "source": [
        "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/shuffled_rdd.png?raw=1\" width=\"40%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "infrared-stations",
      "metadata": {
        "id": "infrared-stations"
      },
      "source": [
        "Above we can see that we started with our shuffled rdd from the previous stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "welsh-sunset",
      "metadata": {
        "id": "welsh-sunset",
        "outputId": "f250e443-565f-4b42-be97-e2e7561857a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M', <pyspark.resultiterable.ResultIterable at 0x10a583a90>),\n",
              " ('S', <pyspark.resultiterable.ResultIterable at 0x10a620730>),\n",
              " ('C', <pyspark.resultiterable.ResultIterable at 0x10a6207c0>),\n",
              " ('P', <pyspark.resultiterable.ResultIterable at 0x10a620850>)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.groupBy(lambda movie: movie[0]).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fallen-growth",
      "metadata": {
        "id": "fallen-growth"
      },
      "source": [
        "And then the next step was our map statement.  And finally the last step was the collect which resulted in Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sitting-uzbekistan",
      "metadata": {
        "id": "sitting-uzbekistan",
        "outputId": "393e5a8d-aab0-48a6-9732-555a0f3d5c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M', ['Minari', 'Michael Clayton']),\n",
              " ('S', ['Shazam!', 'Sicario']),\n",
              " ('C', ['Captain Marvel', 'Casablanca']),\n",
              " ('P', ['Pulp Fiction'])]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.groupBy(lambda movie: movie[0]).map(lambda x : (x[0], list(x[1]))).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-disposal",
      "metadata": {
        "id": "cross-disposal"
      },
      "source": [
        "* Understanding Map partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iraqi-income",
      "metadata": {
        "id": "iraqi-income"
      },
      "source": [
        "Notice that the task in the middle was called map partitions.  As we look at our Spark UI, we'll tend to see this a lot.  Map partitions is quite similar to map.  But map will execute our code one time for each record.  So if we have 2,000 records our related code will be called 2000 times.  With map partitions, the function is called only once per partition.  so if those 2,000 records are divided into 20 partitions, then the related function is only called 20 times.  This is more efficient call.  And it's for this reason that spark will translate our functions into map partitions under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "peripheral-statistics",
      "metadata": {
        "id": "peripheral-statistics"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intelligent-latest",
      "metadata": {
        "id": "intelligent-latest"
      },
      "source": [
        "In this lesson, we learned about Spark performs map reduce, operations that result in shuffling, and how to see these steps using the Spark UI.  We saw that by partitioning our dataset, Spark operations like filter and map across all partitions simultaneously.  And we also saw that one thing to be careful of is shuffling, which occurs when Spark needs to repartition the data to perform the operation.  This often happens with a group by operation.  Shuffling can be expensive because it can require sending data across different nodes in the cluster.\n",
        "\n",
        "We also learned about map partition which is similar to map.  The difference is that instead of performing an opeeration for each record, with map partition Spark performs an operation once per partition, which is faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "received-hundred",
      "metadata": {
        "id": "received-hundred"
      },
      "source": [
        "### Resources\n",
        "\n",
        "[Shuffling Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
        "\n",
        "[5 hour pyspark tutorial](https://www.youtube.com/watch?v=GFC2gOL1p9k&t=2743s)\n",
        "\n",
        "[Map vs Map Partition](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/)\n",
        "\n",
        "[Group by key](https://backtobazics.com/big-data/spark/apache-spark-groupbykey-example/)\n",
        "\n",
        "[Pyspark Google Colab](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}