{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3549dce4",
   "metadata": {
    "id": "lonely-shaft"
   },
   "source": [
    "# Spark In Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e733bdb",
   "metadata": {
    "id": "english-twelve"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c37e7c",
   "metadata": {
    "id": "concerned-examination"
   },
   "source": [
    "If we think about our normal understanding of databases, our databases have two components:\n",
    "\n",
    "1. storage and\n",
    "2. processing\n",
    "\n",
    "That is, we both store data in our databases, and we can perform operations on that data.  \n",
    "\n",
    "As we'll see, with Spark we're mainly concerned with *processing our data*, and not with long term storage.  Instead, our data is stored to disk outside of the database, on a service like S3.  This has vast implications for the way we treat and value data in an organization, as well as the operations we can perform on that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811a574",
   "metadata": {
    "id": "black-holly"
   },
   "source": [
    "### Diving In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0cc1b",
   "metadata": {
    "id": "threaded-hardwood"
   },
   "source": [
    "Let's begin by just diving into Spark to see it in action.  From there we'll get a sense of how it operates differently than our previous databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b14d07",
   "metadata": {
    "id": "expanded-party"
   },
   "source": [
    "> As we move along here, the goal is not to remember details of the steps.  We'll go through each of them in repeated detail later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7ad61",
   "metadata": {
    "id": "protected-links"
   },
   "source": [
    "So first we'll need to install some libraries for reading data from `s3`, and then we'll install Spark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99519a51",
   "metadata": {
    "id": "sealed-differential"
   },
   "outputs": [],
   "source": [
    "# !/Users/jeff/opt/miniconda3/envs/data_engineering/bin/python3.8 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c121bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "silent-investor",
    "outputId": "006685fa-d72f-40d3-e1a4-34db3b7d6dfb"
   },
   "outputs": [],
   "source": [
    "!pip install fsspec --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64e553",
   "metadata": {
    "id": "precise-interest"
   },
   "source": [
    "> Then we run the following to connect to our spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e9984",
   "metadata": {
    "id": "quarterly-sheet"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7f0a0",
   "metadata": {
    "id": "reported-reasoning"
   },
   "source": [
    "Finally, we'll read in data from s3 using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586299e",
   "metadata": {
    "id": "interior-match"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"s3://jigsaw-labs-student/imdb_movies.csv\")\n",
    "df = pd.read_csv('movies.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e98cd",
   "metadata": {
    "id": "decreased-samoa"
   },
   "source": [
    "And from there, we'll turn this dataframe into the equivalent of table in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c588e5",
   "metadata": {
    "id": "entitled-excitement"
   },
   "outputs": [],
   "source": [
    "movies_df = spark.createDataFrame(df.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a4ee0",
   "metadata": {
    "id": "common-burst",
    "outputId": "f5053f4d-6afb-4d0c-bb11-2698c56f13a1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df289c3e",
   "metadata": {
    "id": "qualified-turning"
   },
   "source": [
    "### Spark stores data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56c0f3",
   "metadata": {
    "id": "responsible-washington"
   },
   "source": [
    "Ok, so now let's think about what happened above.  We just connected to our database, created the equivalent of a table to store our movies into, loaded in our data and then selected our first record.  \n",
    "\n",
    "This is a standard workflow in Spark.  We'll use an external resource like S3 for long term storage of our data.  And then we can quickly load in and query our data in Spark.  And unlike previous databases Spark, is primarily leaning on *memory* to process and store our data, as opposed to storing our data on *disk*.\n",
    "\n",
    "We can visualize our workflow in Spark like so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176553cc",
   "metadata": {
    "id": "indirect-canadian"
   },
   "source": [
    "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/s3_to_movies.jpg?raw=1\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d763a2",
   "metadata": {
    "id": "handmade-wonder"
   },
   "source": [
    "> One thing to clear up is that even though we'll use S3 to store our data, Spark will also read in and store that data as well.  But Spark will mainly store our data *in memory*.  And when we are done querying our data, we can shutdown the cluster, which deletes the data from Spark.  And when we perform another query, we'll just read in the data again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089e56e",
   "metadata": {
    "id": "considered-failing"
   },
   "source": [
    "Finally, we should know that Spark is organized as a cluster.  So when Spark reads our data into memory, it can partition it onto different machines in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29236c7e",
   "metadata": {
    "id": "valid-matthew"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/spark_cluster_partition.jpg?raw=1\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a56a53",
   "metadata": {
    "id": "sapphire-logistics"
   },
   "source": [
    "### Contrasting with Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401223bd",
   "metadata": {
    "id": "south-aerospace"
   },
   "source": [
    "So let's take a moment just to appreciate how, by (1) using an external resource for long term storage, and (2) storing data in memory within the cluster, Spark operates differently than other databases like postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95768cb",
   "metadata": {
    "id": "practical-success"
   },
   "source": [
    "With postgres, we store our data within the database and associated records are stored in a file.  When we perform an operation, like selecting records from a table, we then load data into memory and perform any transformations and return the result of that query to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd68f6",
   "metadata": {
    "id": "second-excitement"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/db_both.jpg?raw=1\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7340cd",
   "metadata": {
    "id": "unavailable-draft"
   },
   "source": [
    "As we saw, by the time we load our data into Spark, these records, and our associated tables are primarily in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470442d",
   "metadata": {
    "id": "miniature-submission"
   },
   "source": [
    "### The Benefits of In Memory Storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d3b86",
   "metadata": {
    "id": "literary-afghanistan"
   },
   "source": [
    "So we just saw some of the architecture when working in Spark.  We use an external resource like S3 for long term storage, read our data into Spark which primarily stores our data in memory as opposed to storing the data on disk.  Now let's get into some of the benefits of this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a534d6b",
   "metadata": {
    "id": "assisted-vacation"
   },
   "source": [
    "1. Cheaper storage\n",
    "\n",
    "The first benefit is simply that it costs less money if we store our data externally on something like S3 than storing our data inside of a database on disk.  When our data is on S3, our data is simply written to disk, and there is minimal software running along with it.  By contrast, when we store our data in a database like postgres or redshift, for each node we have running, we are not just paying for the data storage but also a database with the capability to process that data.\n",
    "\n",
    "Because it's cheaper to store data in S3, we can store data that has relatively low value, or even unknown value.  And if a data scientist or data engineer can extract value from it later on, that data will be available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e22067",
   "metadata": {
    "id": "quarterly-dance"
   },
   "source": [
    "2. Schema on Read\n",
    "\n",
    "Not only is the data available, but with storage in memory, it's easier to explore and process this data.  For example, we saw above that it was relatively easy to load in and query our data in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2cd78",
   "metadata": {
    "id": "straight-domain",
    "outputId": "09f586f7-78ec-4343-c425-aed00e222b79"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"s3://jigsaw-labs/imdb_movies.csv\")\n",
    "movies_df = spark.createDataFrame(df.astype(str))\n",
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4e3ad",
   "metadata": {
    "id": "handy-spectrum"
   },
   "source": [
    "Contrast this with loading external data into our standard databases that store data on disk (eg. Postgres, MySQL).  There, to load in data we (1) need to create our tables, (2) transform our data to and (3) insert records into those tables.  With Spark, we can let the software determine the schema that fits to the data.  This is called `schema on read`, as the schema is determined when we read in the data.\n",
    "\n",
    "Also notice that we did not need to really transform the data before loading it into Spark.  For this reason, we do not need to follow the standard ETL pattern when loading in our data.  Instead we can first load our data, and then use Spark to transform it.  This is called Extract Load Transform (or ELT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3a8d4",
   "metadata": {
    "id": "american-facial"
   },
   "source": [
    "3. Memory intensive computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e625839",
   "metadata": {
    "id": "residential-mumbai"
   },
   "source": [
    "The third benefit of memory storage really comes from the perspective of data science.  Performing certain computations in machine learning requires having a large amount of data accessible for computation, and having our entire dataset loaded in memory means that the data will be accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a50ea",
   "metadata": {
    "id": "subsequent-castle"
   },
   "source": [
    "### But there's still local some storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445418f",
   "metadata": {
    "id": "legal-texture"
   },
   "source": [
    "Even though Spark performs much of it's computation in memory, and primarily uses in memory storage, Spark nodes still do use local disks for data that does not fit into RAM, and to store intermediate output in a complex operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa6623",
   "metadata": {
    "id": "experimental-progress"
   },
   "source": [
    "> But this storage on disk is quite minor as compared to most databases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7e98f",
   "metadata": {
    "id": "constant-warren"
   },
   "source": [
    "So let's update our diagram to more accurately reflect the hardware spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb8695",
   "metadata": {
    "id": "exceptional-tourism"
   },
   "source": [
    "> <img src='https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/spark_cluster_disk.jpg?raw=1' width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e02ea4",
   "metadata": {
    "id": "imperial-anxiety"
   },
   "source": [
    "So even though there is technically in local storage to disk Spark, what distinguishes Spark is it's reliance on in memory storage and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e9b0f",
   "metadata": {
    "id": "asian-michigan"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4ed75",
   "metadata": {
    "id": "humanitarian-exception"
   },
   "source": [
    "In this lesson, we saw that with Spark we store our data externally in something like an S3 bucket, and then load this data into memory when used by Spark.  If we need additional memory, we add more nodes to our Spark cluster.\n",
    "\n",
    "Because Spark is primarily uses memory storage, this means that we are able to store data with low value, or unknown value.  And reading our data into memory means that we do not need to first create our data, transform our data and then load our data into these tables (ETL) but rather can load and then transform ouor data.  Finally, we saw that for machine learning, Spark is useful for memory intensive calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb6276",
   "metadata": {},
   "source": [
    "<a href=\"1_pyspark_cluster_lab.ipynb\" style=\"background-color:blue;color:white;padding:10px;margin:2px;font-weight:bold;\">Next Notebook</a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
