{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7ae925",
   "metadata": {
    "id": "smooth-possibility"
   },
   "source": [
    "# Setting up a Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d379b7",
   "metadata": {
    "id": "entertaining-ensemble"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8655d0e",
   "metadata": {
    "id": "collected-limitation"
   },
   "source": [
    "In this lesson, we'll practice using Spark to change the datatypes of our schema.  We'll do so using the database of flood insurance claims.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed7ce0",
   "metadata": {
    "id": "adverse-liberia"
   },
   "source": [
    "### Getting Set Up (For Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8941d5f",
   "metadata": {
    "id": "unique-lucas"
   },
   "source": [
    "> If we are running this on google colab, we can run the following to eventually interact with our Spark UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a676f7",
   "metadata": {
    "id": "designed-edgar"
   },
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef11e24",
   "metadata": {
    "id": "blocked-hartford"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed70753",
   "metadata": {
    "id": "fabulous-database"
   },
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641baaa",
   "metadata": {
    "id": "continental-pregnancy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40de1b",
   "metadata": {
    "id": "introductory-bundle"
   },
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032c119",
   "metadata": {
    "id": "opponent-connectivity"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"civComplaints\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8168d7",
   "metadata": {
    "id": "lined-dining"
   },
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8514a",
   "metadata": {
    "id": "innovative-plaza"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee16061",
   "metadata": {
    "id": "numerous-private"
   },
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61222f9",
   "metadata": {
    "id": "answering-sheriff"
   },
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5acc46",
   "metadata": {
    "id": "complicated-pipeline"
   },
   "source": [
    "### Setting DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758c523",
   "metadata": {
    "id": "raised-cleaning"
   },
   "source": [
    "Then let's load our data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e09e23",
   "metadata": {
    "id": "dominant-security"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('s3://jigsaw-labs/houston_claims.csv', index_col = 0).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5f020",
   "metadata": {
    "id": "endless-netherlands"
   },
   "source": [
    "And then load the data into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e34c10",
   "metadata": {
    "id": "qualified-kitchen"
   },
   "outputs": [],
   "source": [
    "claims_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dcc5b",
   "metadata": {
    "id": "continental-buying"
   },
   "source": [
    "Next use the `dtypes` method to see the format of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0d793",
   "metadata": {
    "id": "crucial-treasure",
    "outputId": "d69ffcef-c977-46a1-9fb0-7f97f2dd6004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'string'),\n",
       " ('amountPaidOnContentsClaim', 'string'),\n",
       " ('yearofLoss', 'string'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [('reportedCity', 'string'),\n",
    "#  ('dateOfLoss', 'string'),\n",
    "#  ('elevatedBuildingIndicator', 'string'),\n",
    "#  ('floodZone', 'string'),\n",
    "#  ('latitude', 'string'),\n",
    "#  ('longitude', 'string'),\n",
    "#  ('lowestFloodElevation', 'string'),\n",
    "#  ('amountPaidOnBuildingClaim', 'string'),\n",
    "#  ('amountPaidOnContentsClaim', 'string'),\n",
    "#  ('yearofLoss', 'string'),\n",
    "#  ('reportedZipcode', 'string'),\n",
    "#  ('id', 'string')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c017d",
   "metadata": {
    "id": "considerable-campus"
   },
   "source": [
    "So we can see that multiple columns are not in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a1dd1",
   "metadata": {
    "id": "modified-campbell"
   },
   "source": [
    "Begin by making the following changes. \n",
    "* Change `latitude` and `longitude` into floats.\n",
    "> Use the [spark documentation](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e8da4",
   "metadata": {
    "id": "broadband-announcement"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76275c7",
   "metadata": {
    "id": "seven-binary"
   },
   "source": [
    "If we check the datatypes of `latitude` `longitude` and `id`, we should see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75801447",
   "metadata": {
    "id": "extensive-council",
    "outputId": "2d5055db-e6b5-4cac-a684-26d2c5e9efa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: float, longitude: float]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude'])\n",
    "# DataFrame[latitude: float, longitude: float, id: int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083007c",
   "metadata": {
    "id": "damaged-asthma"
   },
   "source": [
    "And we should see the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d00c7d",
   "metadata": {
    "id": "infrared-fraction",
    "outputId": "22cec3a2-fe2b-4991-d923-89c42ed27332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|latitude|longitude|\n",
      "+--------+---------+\n",
      "|    29.7|    -95.5|\n",
      "|    29.5|    -95.1|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude']).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a260f9",
   "metadata": {
    "id": "departmental-fetish"
   },
   "source": [
    "Of course, we still have some additional columns that we should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242a47a",
   "metadata": {
    "id": "divided-parker",
    "outputId": "685f6ee0-a2cf-4609-a95d-5cff0f58775f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reportedCity: string (nullable = true)\n",
      " |-- dateOfLoss: string (nullable = true)\n",
      " |-- elevatedBuildingIndicator: string (nullable = true)\n",
      " |-- floodZone: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- lowestFloodElevation: string (nullable = true)\n",
      " |-- amountPaidOnBuildingClaim: string (nullable = true)\n",
      " |-- amountPaidOnContentsClaim: string (nullable = true)\n",
      " |-- yearofLoss: string (nullable = true)\n",
      " |-- reportedZipcode: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d94af8",
   "metadata": {
    "id": "documentary-appliance"
   },
   "source": [
    "So now let's make the following changes to the `amountPaid` and `yearOfLoss` columns:\n",
    "* Change the `amountPaidOnBuildingClaim` and columns `amountPaidOnContentsClaim` into floats, and rename the columns to `amount_paid_bldg`, and `amount_paid_contents` respectively.   \n",
    "* And change the `yearOfLoss` column into an integer, renaming the column to `year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed4a79",
   "metadata": {
    "id": "processed-bathroom"
   },
   "outputs": [],
   "source": [
    "updated_renamed_claims_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58133c",
   "metadata": {
    "id": "instructional-requirement"
   },
   "source": [
    "And then let's take a look at this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7947ee9",
   "metadata": {
    "id": "reflected-mainstream",
    "outputId": "36ecdd85-926c-4a0f-fb9d-8012035eb0a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amount_paid_bldg', 'int'),\n",
       " ('amount_paid_contents', 'int'),\n",
       " ('year', 'int'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_renamed_claims_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1aeaa0",
   "metadata": {
    "id": "ignored-lemon"
   },
   "source": [
    "Ok, so the remaining column to change is the `dateOfLoss` column.  Let's tackle this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478ce0",
   "metadata": {
    "id": "interracial-letters"
   },
   "source": [
    "### Changing the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72805d48",
   "metadata": {
    "id": "accompanied-waterproof"
   },
   "source": [
    "Finally change the date of loss column.  If we look at it, we can see that it has a full time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43120a70",
   "metadata": {
    "id": "massive-polish",
    "outputId": "dda5721c-501a-49ba-fa18-d5ce7edf74c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          dateOfLoss|\n",
      "+--------------------+\n",
      "|2017-08-27T00:00:...|\n",
      "|2008-09-12T00:00:...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_renamed_claims_df.select('dateOfLoss').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc5bc4",
   "metadata": {
    "id": "conceptual-yugoslavia"
   },
   "source": [
    "So we can change this column by renaming it from `dateOfLoss` to simply `date`, and changing the datatype to be of type `TimeStampType`.  \n",
    "\n",
    "> We do not need to use the `to_date` method here, as the `TimeStampType` can interpret this formatting out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312628b",
   "metadata": {
    "id": "broke-korean"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56826a87",
   "metadata": {
    "id": "fitted-church"
   },
   "outputs": [],
   "source": [
    "coerced_claims_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c9cda",
   "metadata": {
    "id": "cheap-money",
    "outputId": "1dbf9a98-e80c-4b61-f03e-3184d36773c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2017-08-26 20:00:00|\n",
      "|2008-09-11 20:00:00|\n",
      "|2004-06-28 20:00:00|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coerced_claims_df.select('date').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f20e5c",
   "metadata": {
    "id": "destroyed-nature"
   },
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9b036",
   "metadata": {
    "id": "prerequisite-domestic"
   },
   "source": [
    "Now that we have formatted our data we can begin to use SQL.  Do so by setting the name of the temporary view for our dataframe to `claims`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac6b15",
   "metadata": {
    "id": "established-screening"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bf91a30",
   "metadata": {
    "id": "extended-belly"
   },
   "source": [
    "And then display the months that the first two claims were made, setting an alias of `month_of_claim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612b721",
   "metadata": {
    "id": "proof-tragedy",
    "outputId": "51f168b1-92ae-4a74-dd40-031630250e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|year_of_claim|\n",
      "+-------------+\n",
      "|         2017|\n",
      "|         2008|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b874b80",
   "metadata": {
    "id": "helpful-musical"
   },
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a347a",
   "metadata": {
    "id": "dutch-plasma"
   },
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)\n",
    "\n",
    "[Spark Tricks Blog](https://georgheiler.com/2019/02/22/dynamically-select-columns-by-type/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786151e1",
   "metadata": {},
   "source": [
    "<a href=\"index.ipynb\" style=\"background-color:blue;color:white;padding:10px;margin:2px;font-weight:bold;\">Next Notebook</a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
