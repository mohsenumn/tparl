{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31dea1d1",
   "metadata": {
    "id": "macro-length"
   },
   "source": [
    "# Parallel Processing with Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878f20d",
   "metadata": {
    "id": "black-kentucky"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe3302",
   "metadata": {
    "id": "governing-constitutional"
   },
   "source": [
    "As we know, Spark allows us to store data in memory in a distributed cluster of nodes.  As we'll see in this lesson, that cluster is organized with a **driver node**, which is our entry point into the cluster, as well as **worker nodes**, where tasks are carried out on the various partitions of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77638c78",
   "metadata": {
    "id": "outstanding-johns"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/spark_cluster_disk.jpg?raw=1\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59664a18",
   "metadata": {
    "id": "emerging-nursery"
   },
   "source": [
    "It's really the worker nodes, whose software is called an executor, where both partitioning of the data, and simultaneous querying of those partitions occurs.  In this lesson, we'll take a deeper look at these worker nodes, and see how they are able to allow us to process data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67239f5",
   "metadata": {
    "id": "finnish-apparatus"
   },
   "source": [
    "### Creating the Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539263c",
   "metadata": {
    "id": "protective-express"
   },
   "source": [
    "Before we can get to our executors, the first step we need to perform is create our Spark Context.   Our Spark context, is how we interact with our driver, which is our entrypoint into our Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2866ff3",
   "metadata": {
    "id": "juvenile-nebraska"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eac0c",
   "metadata": {
    "id": "existing-deposit"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e95ef",
   "metadata": {
    "id": "animated-photograph"
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b135a5",
   "metadata": {
    "id": "talented-jurisdiction"
   },
   "source": [
    "> Now with the code above, we just set the name of our Spark cluster, `films` and we specified that we will be running our cluster on our local computer (as opposed to something like AWS), and that our cluster will have two cores (with `local[2]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45ab09",
   "metadata": {
    "id": "american-imperial"
   },
   "source": [
    "That last component is quite important.  This is because, with Spark if one main feature is storage in memory, the other is that Spark performs our queries in parallel.  And by setting the number of cores in our cluster we are determining the amount of parallelization when we perform our queries.\n",
    "\n",
    "We'll focus on this feature of Spark in this lesson, but to do so we'll need to take a deeper look at the worker nodes -- where all of this parallelization occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e50b9",
   "metadata": {
    "id": "guided-coach"
   },
   "source": [
    "### Looking at an Executor: CPU Cores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fab78",
   "metadata": {
    "id": "expanded-world"
   },
   "source": [
    "As we know, when we read our data into Spark, we can store that data distributed through worker nodes located in the cluster.  \n",
    "\n",
    "> Now in Spark, *the software* operating on each worker node is referred to as an **executor**. The computer is the worker node.  Each orange box above represents an executor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934886d",
   "metadata": {
    "id": "dependent-facial"
   },
   "source": [
    "So below we can see that when we read in our movies dataset from the `s3` bucket, these records are distributed across the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89913d",
   "metadata": {
    "id": "tribal-barcelona"
   },
   "source": [
    "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/cluster_executor.jpg?raw=1\" width='80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb97668",
   "metadata": {
    "id": "chemical-intensity"
   },
   "source": [
    "Now above we see that our dataset is partitioned into three different partitions, one for each node.  And then when we say look for a specific record, each executor will get to work looking for the data.  But really, on each executor, we often have multiple partitions of our data -- and what constrains our ability to partition in our data is the number of cores in all of executors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33113148",
   "metadata": {
    "id": "sticky-journal"
   },
   "source": [
    "If we think about it, it makes sense that the number of partitions of our data is determined by the number of cores.  This is because even though we may only have one CPU doing the work in an executor, if there are multiple cores, then multiple processes can be performed simultaneously.  And because our data is in memory, we can partition our dataset, so that each core can process a subdivision of the data.  \n",
    "\n",
    "So when we look at an executor, a main thing to consider is the number of cores available to us to partition our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436cae6",
   "metadata": {
    "id": "expressed-child"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/executor_closer.jpg?raw=1\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f87fb",
   "metadata": {
    "id": "pressed-professor"
   },
   "source": [
    "### Seeing Parallelization in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1397b0",
   "metadata": {
    "id": "adequate-capacity"
   },
   "source": [
    "We can get a sense of this parallelization if we pass our data into Spark.  In fact one way to feed our data into spark is with a method called parallelize. \n",
    "\n",
    "> For example, we can start with a list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7515f",
   "metadata": {
    "id": "terminal-margin"
   },
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Captain Marvel', 'Escape Room', 'How to Train Your Dragon: The Hidden World']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b33a22",
   "metadata": {
    "id": "thrown-amazon"
   },
   "source": [
    "And from there, we move this data into Spark with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949a59",
   "metadata": {
    "id": "suitable-scheme",
    "outputId": "de19f850-702a-4634-a063-7e4067e19427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = sc.parallelize(movies)\n",
    "movies_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6480c",
   "metadata": {
    "id": "canadian-habitat"
   },
   "source": [
    "> And then we can look at the number of cores in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e1bbd",
   "metadata": {
    "id": "union-tribune",
    "outputId": "ad25adf7-a98e-40ae-803d-93019b0a56cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8217779",
   "metadata": {
    "id": "fiscal-disclosure"
   },
   "source": [
    "So we can see that Spark automatically partitioned our data, with one partition for each core in the cluster.\n",
    "\n",
    "> Remember that we specified the number of cores when we create our spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9349fe",
   "metadata": {
    "id": "quiet-xerox"
   },
   "source": [
    "```python\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc96adb",
   "metadata": {
    "id": "complimentary-carbon"
   },
   "source": [
    "The importance of this is now we can perform operations across four different partitions of the dataset simultanously.  So if we want to find a matching film record, we can search across four different partitions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97371c",
   "metadata": {
    "id": "civilian-differential",
    "outputId": "84948cb5-c7f3-4663-cac7-791767b28ba2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain Marvel']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.filter(lambda movie: movie == 'Captain Marvel').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53784456",
   "metadata": {
    "id": "frank-diameter"
   },
   "source": [
    "> So in the line above, our data was first split up four ways, and then we looked for Captain Marvel on each slice of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521abfd",
   "metadata": {
    "id": "silver-birth"
   },
   "source": [
    "### Looking at an Executor: Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a4a6e",
   "metadata": {
    "id": "residential-handling"
   },
   "source": [
    "So we just saw that one piece of hardware that can determine the processing of our data is the number of cores in per executor.  Another way the hardware of an executor can constrain how we process data is with the available memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ba03f",
   "metadata": {
    "id": "intimate-lover"
   },
   "source": [
    "<img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/executor_closer.jpg?raw=1\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385cacc",
   "metadata": {
    "id": "faced-ministry"
   },
   "source": [
    "The reason why memory can impact our *processing* of data, is because our memory in an executor is divided into *working memory* and *storage*.\n",
    "\n",
    "> And by default, both working and storage are allocated 50% of total memory on a node.  \n",
    "\n",
    "And this is important to consider because if we decide to persist our data in memory, we may consume some of the working memory needed to say filter through our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb9209",
   "metadata": {
    "id": "endangered-scotland"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10905f0",
   "metadata": {
    "id": "black-handling"
   },
   "source": [
    "In this lesson, we saw how Spark can allow us to process our data in parallel through our executors.  In Spark we have one executor operating on each worker node, and those executors have one or more CPUs which have one or more cores.  Spark partitions our data and allocate a partition to each core on a cluster.  Then when we run a query, like looking for a record, each core can query it's partitioned data.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c80e21",
   "metadata": {
    "id": "undefined-postcard"
   },
   "source": [
    "So we saw that the parallel processing capabilities of our Spark cluster is constrained by the number of cores across our cluster's executors.  We also saw that this processing is constrained by the amount of available memory in each executor.  This is because in Spark, memory on each executor is divided into memory for storage of data, and working memory -- needed when performing queries.  And when too much memory is consumed with storage it can leave less data needed when processing that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98d7cc",
   "metadata": {
    "id": "constant-ethics"
   },
   "source": [
    "Finally, in this lesson, we saw how to create a cluster, parallelize our data, and query a cluster.  We created our cluster with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0db2a8",
   "metadata": {
    "id": "civilian-blake"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13890695",
   "metadata": {
    "id": "incorporate-resort"
   },
   "source": [
    "> By specifying `setMaster(\"local[2]\")`, above we said specified to distribute our tasks across two different cores.  \n",
    "\n",
    "And now with our Spark Context set up, we could feed a Python list into a dataset distributed across our Spark Cluster with the `parallelize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a41b97",
   "metadata": {
    "id": "according-finance"
   },
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Captain Marvel', 'Escape Room', 'How to Train Your Dragon: The Hidden World']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a827cc",
   "metadata": {
    "id": "accepted-appendix"
   },
   "outputs": [],
   "source": [
    "movies_rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9298372",
   "metadata": {
    "id": "alleged-highway"
   },
   "source": [
    "And we can see the number of partitions with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94483605",
   "metadata": {
    "id": "arctic-bibliography",
    "outputId": "d472a5c9-0817-471f-aa4c-941a850c796f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281e6df",
   "metadata": {
    "id": "described-train"
   },
   "source": [
    "Finally, we used filter to search through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84334c96",
   "metadata": {
    "id": "reliable-amino",
    "outputId": "23db18f5-59b4-489a-9cca-419662828e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain Marvel']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.filter(lambda movie: movie == 'Captain Marvel').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747485a",
   "metadata": {
    "id": "creative-official"
   },
   "source": [
    "### Resources\n",
    "\n",
    "* [Spark Internals Gitbook](https://books.japila.pl/apache-spark-internals/overview/)\n",
    "\n",
    "* [Drivers and Executors Knoldus Blog](https://blog.knoldus.com/understanding-the-working-of-spark-driver-and-executor/)\n",
    "\n",
    "* [Drivers and Executors StackOverflow](https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster)\n",
    "\n",
    "* [Presenting RDDs Berkeley Paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "* [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)\n",
    "\n",
    "* [Databricks best practices gitbook](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845cb0f",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"2_map_reduce_and_shuffle.ipynb\" style=\"background-color:blue;color:white;padding:10px;margin:2px;font-weight:bold;\">Next Notebook</a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
